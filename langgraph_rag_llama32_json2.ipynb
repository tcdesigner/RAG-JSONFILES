{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cece98f-a3ed-417e-8b6a-1754e8f9c42a",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Local RAG agent with LLaMA3\n",
    "We'll combine ideas from paper RAG papers into a RAG agent:\n",
    "\n",
    "**Routing**: Adaptive RAG (paper). Route questions to different retrieval approaches\n",
    "\n",
    "**Fallback**: Corrective RAG (paper). Fallback to web search if docs are not relevant to query\n",
    "\n",
    "**Self-correction**: Self-RAG (paper). Fix answers w/ hallucinations or donâ€™t address question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f268c6",
   "metadata": {},
   "source": [
    "![alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406dea2",
   "metadata": {},
   "source": [
    "# Local models\n",
    "Embedding\n",
    "\n",
    "GPT4All Embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ca755",
   "metadata": {},
   "source": [
    "pip install langchain-nomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4140de",
   "metadata": {},
   "source": [
    "**LLM**\n",
    "\n",
    "Use Ollama and llama3.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6e4a7",
   "metadata": {},
   "source": [
    "ollama pull llama3.2:3b-instruct-fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083efbb9",
   "metadata": {},
   "source": [
    "**Search**\n",
    "\n",
    "For search, we use Tavily, which is a search engine optimized for LLMs and RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea269f6",
   "metadata": {},
   "source": [
    "**Tracing**\n",
    "\n",
    "Optionally, use LangSmith for tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fcaba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain langchain_community tiktoken langchain-nomic \"nomic[local]\" langchain-ollama scikit-learn langgraph tavily-python bs4\n",
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "#_set_env(\"TAVILY_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"local-llama32-rag\"\n",
    "\n",
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8466b1",
   "metadata": {},
   "source": [
    "**Vectorstore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Subjects in the Vectorstore\n",
    "subject = \"BIOS test results, BIOS test performance, metric on X EPYC Platform\"\n",
    "# Paths to JSON files\n",
    "json_files = [\n",
    "    r\"C:\\Users\\Tim\\Projects\\EPDW-AI-main\\TPC-C_MARIADB_ten_bios_benchmark_results-1.json\",\n",
    " #   r\"C:\\Users\\Tim\\Projects\\EPDW-AI-main\\TPC-C_MARIADB_ten_bios_benchmark_results-2.json\",\n",
    " #   r\"C:\\Users\\Tim\\Projects\\EPDW-AI-main\\TPC-C_MARIADB_ten_bios_benchmark_results-3.json\",\n",
    "]\n",
    "\n",
    "# Load documents from JSON files\n",
    "def load_json_documents(file_paths):\n",
    "    docs = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "            # Assuming each JSON file contains a list of documents\n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    docs.append(Document(page_content=item if isinstance(item, str) else str(item), metadata={}))\n",
    "            else:\n",
    "                docs.append(Document(page_content=data if isinstance(data, str) else str(data), metadata={}))\n",
    "    return docs\n",
    "\n",
    "docs_list = load_json_documents(json_files)\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab90f9",
   "metadata": {},
   "source": [
    "**Web Search Tool**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb057a5",
   "metadata": {},
   "source": [
    "API Reference: TavilySearchResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a958de5",
   "metadata": {},
   "source": [
    "## Graph\n",
    "We build the above workflow as a graph using LangGraph.\n",
    "\n",
    "**Graph state**\n",
    "\n",
    "The graph state schema contains keys that we want to:\n",
    "\n",
    "Pass to each node in our graph\n",
    "Optionally, modify in each node of our graph\n",
    "See conceptual docs here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532ca149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n",
    "    \"\"\"\n",
    "    #subject: str  # data in vectorsotre\n",
    "    question: str  # User question\n",
    "    generation: str  # LLM generation\n",
    "    web_search: str  # Binary decision to run web search\n",
    "    max_retries: int  # Max number of retries for answer generation\n",
    "    answers: int  # Number of answers generated\n",
    "    loop_step: Annotated[int, operator.add]\n",
    "    documents: List[str]  # List of retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1833e8d",
   "metadata": {},
   "source": [
    "Each node in our graph is simply a function that:\n",
    "\n",
    "(1) Take state as an input\n",
    "\n",
    "(2) Modifies state\n",
    "\n",
    "(3) Write the modified state to the state schema (dict)\n",
    "\n",
    "See conceptual docs here.\n",
    "\n",
    "Each edge routes between nodes in the graph.\n",
    "\n",
    "See conceptual docs here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1120682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langgraph.graph import END\n",
    "\n",
    "### Router\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "### Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Args: The current graph state.\n",
    "    Main code Logic: With question, retrieve relavant documents from vectorstore.\n",
    "    Returns: A dict which let langgraph update the graph state\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "  \n",
    "    # retrieved documents in vectorstore with question\n",
    "    docs = retriever.invoke(state[\"question\"])\n",
    "    # update the graph state with retrieved documents\n",
    "    return {\"documents\": docs}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "    Args: state (dict): The current graph state.\n",
    "    Returns: state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "    \n",
    "    # Doc grader instructions\n",
    "    doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "    # Grader prompt\n",
    "    doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}.\n",
    "     This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "    Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in state[\"documents\"]:\n",
    "        doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "            document=d.page_content, question=state[\"question\"]\n",
    "        )\n",
    "        result = llm_json_mode.invoke(\n",
    "            [SystemMessage(content=doc_grader_instructions)]\n",
    "            + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "        )\n",
    "        grade = json.loads(result.content)[\"binary_score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    # Prompt\n",
    "    rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Here is the context to use to answer the question:\n",
    "    {context} \n",
    "    Think carefully about the above context. \n",
    "    Now, review the user question:\n",
    "    {question}\n",
    "    Provide an answer to this questions using only the above context. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    print(\"---GENERATE---\")\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "\n",
    "    # RAG generation\n",
    "    docs_txt = format_docs(state[\"documents\"])\n",
    "    rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=state[\"question\"])\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"generation\": generation, \"loop_step\": loop_step + 1}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": state[\"question\"]})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "### Edges\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "    The vectorstore contains documents related to {subject}.\n",
    "    Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "    Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "    router_instructions_formatted = router_instructions.format( subject=subject )\n",
    "    print(router_instructions_formatted)\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    route_question = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=router_instructions_formatted)]\n",
    "        + [HumanMessage(content=state[\"question\"])]\n",
    "    )\n",
    "    source = json.loads(route_question.content)[\"datasource\"]\n",
    "    if source == \"websearch\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    \n",
    "    if state[\"web_search\"] == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: NOT ALL DOCUMENTS ARE RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    max_retries = state.get(\"max_retries\", 3)  # Default to 3 if not provided\n",
    "\n",
    "    # Hallucination grader instructions\n",
    "    hallucination_grader_instructions = \"\"\"\n",
    "    You are a teacher grading a quiz. \n",
    "    You will be given FACTS and a STUDENT ANSWER.\n",
    "    Here is the grade criteria to follow:\n",
    "    (1) Ensure the STUDENT ANSWER is grounded in the FACTS.\n",
    "    (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "    Score:\n",
    "    A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score.\n",
    "    A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "    Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.\n",
    "    Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "    # Grader prompt\n",
    "    hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "    Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "\n",
    "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
    "        documents=format_docs(state[\"documents\"]), generation=state[\"generation\"].content\n",
    "    )\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=hallucination_grader_instructions)]\n",
    "        + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
    "    )\n",
    "    grade = json.loads(result.content)[\"binary_score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        # Answer grader instructions\n",
    "        answer_grader_instructions = \"\"\"You are a teacher grading a quiz.\n",
    "        You will be given a QUESTION and a STUDENT ANSWER.\n",
    "        Here is the grade criteria to follow:\n",
    "        (1) The STUDENT ANSWER helps to answer the QUESTION\n",
    "        Score:\n",
    "        A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score.\n",
    "        The student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.\n",
    "        A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "        Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.\n",
    "        Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "        # Grader prompt\n",
    "        answer_grader_prompt = \"\"\"QUESTION: \\n\\n {question} \\n\\n STUDENT ANSWER: {generation}.\n",
    "        Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "        \n",
    "        # Test using question and generation from above\n",
    "        answer_grader_prompt_formatted = answer_grader_prompt.format(\n",
    "            question=state[\"question\"], generation=state[\"generation\"].content\n",
    "        )\n",
    "        result = llm_json_mode.invoke(\n",
    "            [SystemMessage(content=answer_grader_instructions)]\n",
    "            + [HumanMessage(content=answer_grader_prompt_formatted)]\n",
    "        )\n",
    "        grade = json.loads(result.content)[\"binary_score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        elif state[\"loop_step\"] <= max_retries:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "            return \"max retries\"\n",
    "    elif state[\"loop_step\"] <= max_retries:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"hallucination\"\n",
    "    else:\n",
    "        print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "        return \"max retries\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a252c",
   "metadata": {},
   "source": [
    "API Reference: Document | END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c450b45",
   "metadata": {},
   "source": [
    "# Control Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generate\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"hallucination\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "        \"max retries\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17766a86",
   "metadata": {},
   "source": [
    "API Reference: StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcde626",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"What are the BIOS test result for RUN 1?\", \"max_retries\": 3\n",
    "           }\n",
    "\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbdc44e",
   "metadata": {},
   "source": [
    "Trace:\n",
    "https://smith.langchain.com/public/1e01baea-53e9-4341-a6d1-b1614a800a97/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on current events\n",
    "inputs = {\n",
    "    \"question\": \"What are the models released today for llama3.2?\",\n",
    "    \"max_retries\": 3,\n",
    "}\n",
    "\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd029f2",
   "metadata": {},
   "source": [
    "Trace:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755eec41",
   "metadata": {},
   "source": [
    "https://smith.langchain.com/public/acdfa49d-aa11-48fb-9d9c-13a687ff311f/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
